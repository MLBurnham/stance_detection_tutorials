{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34c3946c",
   "metadata": {},
   "source": [
    "# Stance Detection: Hyperparameter sweeps with Weights and Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febe675b",
   "metadata": {},
   "source": [
    "#### This tutorial demonstrates how to train a transformer with hyperparameter sweep. Our task is to classify support for Trump in a data set of tweets, and we will use a RoBERTa model that has been domain adapted for classification of political tweets.\n",
    "\n",
    "#### Requirements:\n",
    "1. A basic understanding of Python.\n",
    "2. Access to a GPU (free services like Google Colab work well).\n",
    "3. An account with [Weights and Biases](https://wandb.ai/site).\n",
    "\n",
    "#### For additional resources, consult the documentation.\n",
    "[Simple Transformers](https://simpletransformers.ai/docs/installation/)\n",
    "\n",
    "[Weights and Biases](https://docs.wandb.ai/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0551cbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import pandas as pd\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "from simpletransformers.classification import ClassificationArgs\n",
    "import torch\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea64c6cd",
   "metadata": {},
   "source": [
    "First we need to check that we can properly communicate with the GPU. If the code prints \"Cuda available\", everything is working as expected. If you are using Google Colab and this returns CPU, then click on \"Runtime\" at the top of the page, select \"Change Runtime\", and then select \"GPU\" under \"Hardware Accelerator.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c3bc97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available\n",
      "PyTorch version:  1.10.2\n"
     ]
    }
   ],
   "source": [
    "print(\"Cuda available\" if torch.cuda.is_available() is True else \"CPU\")\n",
    "print(\"PyTorch version: \", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc548b42",
   "metadata": {},
   "source": [
    "For this example we will use the same training and test data as in [Burnham (2022)](https://drive.google.com/file/d/1LAbQ0zzBqXImq-Go38bSp6AqVvvYFwU0/view?usp=sharing). The data set consists of tweets about President Trump that are manually labeled 1: approve, 0: not approve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a9e0377",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('https://raw.githubusercontent.com/MLBurnham/stance_detection_tutorials/main/data/train.csv')\n",
    "test_df = pd.read_csv('https://raw.githubusercontent.com/MLBurnham/stance_detection_tutorials/main/data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d822483c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@realDonaldTrump  I like Mexicans who come to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @AhmedtheBanker: Let's not forget @realDona...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@realDonaldTrump did not apply to immigrants o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Been slacking on my @realDonaldTrump retweets....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>And how many  #latinos enemies you gained in 1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>yes @USER preach !!!! america is listing!!!!!#...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131</th>\n",
       "      <td>tonight i will go to bed thinking about the ab...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1132</th>\n",
       "      <td>@USER \\n\\ncome ny harlem, long island...reassu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1133</th>\n",
       "      <td>@USER trump battered that scary boogie man bid...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134</th>\n",
       "      <td>so did chris wallace or @USER win the debate??...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1135 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  labels\n",
       "0     @realDonaldTrump  I like Mexicans who come to ...       1\n",
       "1     RT @AhmedtheBanker: Let's not forget @realDona...       0\n",
       "2     @realDonaldTrump did not apply to immigrants o...       0\n",
       "3     Been slacking on my @realDonaldTrump retweets....       1\n",
       "4     And how many  #latinos enemies you gained in 1...       0\n",
       "...                                                 ...     ...\n",
       "1130  yes @USER preach !!!! america is listing!!!!!#...       1\n",
       "1131  tonight i will go to bed thinking about the ab...       0\n",
       "1132  @USER \\n\\ncome ny harlem, long island...reassu...       1\n",
       "1133  @USER trump battered that scary boogie man bid...       1\n",
       "1134  so did chris wallace or @USER win the debate??...       0\n",
       "\n",
       "[1135 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8cfed7",
   "metadata": {},
   "source": [
    "This will login to Weights and Biases so that we can send results to our account and track our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bbefae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\mikeb/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834fa083",
   "metadata": {},
   "source": [
    "This chunk of code configues our sweep parameters. We can define how we want to search the hyperparameter space (bayes, grid, or random), as well as the range of parameters we want to search. If using a bayesian sweep, we can choose the performance metric we want to optimize for. In this case, I'm using a bayesian sweep to maximize MCC. \n",
    "\n",
    "Generally, the most important parameters to search are the number of training epochs (how many times the training data passes through the model) and the learning rate (the size of the changes implement when the models adjusts its weights and biases during training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a082d11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\"name\": \"mcc\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"num_train_epochs\": {\"min\": 0, \"max\": 10},\n",
    "        \"learning_rate\": {\"min\": 0.0, \"max\": 5e-04 },\n",
    "    },\n",
    "#    \"early_terminate\": {\"type\": \"hyperband\", \"min_iter\":6,}, # early terminate can be used to stop runs of the model that have turned degenerate.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b975fa6",
   "metadata": {},
   "source": [
    "Using our configuration, we can now initialize the sweep. This will create a project on your Weights and Biases account and pass your sweep configuration to that project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "448ec1e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: ktzjgwry\n",
      "Sweep URL: https://wandb.ai/mlburnham/stance_sweep/sweeps/ktzjgwry\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"stance_sweep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e8129a",
   "metadata": {},
   "source": [
    "Next we define the specific parameters we want to use in the actual model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99a282f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = ClassificationArgs()\n",
    "\n",
    "model_args.evaluate_during_training = True # Whether or not the model should periodically stop to evaluate performance during training\n",
    "model_args.evaluate_during_training_silent = True\n",
    "model_args.evaluate_during_training_steps = 10 # How frequently the model should stop\n",
    "model_args.manual_seed = 1 # Random seed\n",
    "model_args.max_seq_length = 512 # Max sequence length is the maximum number of tokens the model can accept. Anything beyond the max length is truncated.\n",
    "model_args.train_batch_size = 16 # The batch size is the number of examples that are passed through the model at a time. Larger batch sizes train faster, but require more VRAM on the GPU.\n",
    "model_args.eval_batch_size = 16\n",
    "model_args.train_custom_parameters_only = False\n",
    "model_args.wandb_project = \"stance_sweep\"\n",
    "# Adjust these args if you want to save local copies of the model. This is generally not recommended as models are large and many models can quickly fill all of your disk space.\n",
    "model_args.no_cache = True\n",
    "model_args.no_save = True\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.reprocess_input_data = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43555fda",
   "metadata": {},
   "source": [
    "The final step before starting the sweep is to define our training function. Within the function we choose which model we are using, pass the model and sweep configuration, and tell the model which data to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff5bf197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function\n",
    "def train():\n",
    "    # Initialize a new wandb run\n",
    "    wandb.init(resume = True)\n",
    "\n",
    "    # Create a TransformerModel\n",
    "    model = ClassificationModel(\n",
    "        model_type = \"bertweet\", # Model type refers to the base version of the model. In this case we are using BERTweet, a RoBERTa model trained on Twitter data.\n",
    "        \n",
    "        # Model name refers to the specific version of the model we want to use. \n",
    "        # In this case, we are using a version of BERTweet that has been domain adapted for political tweets found here: https://huggingface.co/kornosk/polibertweet-political-twitter-roberta-mlm.\n",
    "        # You can use different models by pointing it to a different model on the huggingface model repository.\n",
    "        model_name = \"kornosk/polibertweet-political-twitter-roberta-mlm\", \n",
    "        weight = [1,3], # We're going to use weights since our data set is unbalanced, there are about three times as many not support tweets as support. This will weight missclassification of support tweets more heavily when calculating the model's loss or error.\n",
    "        use_cuda=True, # Use the GPU\n",
    "        args=model_args, # pass model arguments we defined above to the function\n",
    "        sweep_config=wandb.config, # Pass sweep configuration we defined above to the function\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model.train_model(\n",
    "        train_df,\n",
    "        eval_df=test_df,\n",
    "        verbose = False,\n",
    "        accuracy=lambda truth, predictions: accuracy_score(\n",
    "            truth, [round(p) for p in predictions]\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Sync with W&B\n",
    "    wandb.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8680062d",
   "metadata": {},
   "source": [
    "Now we start training the model by telling the wandb agent which sweep to run, which training function to use, and how many iterations of the model it should train. Results should update in realtime on your W&B account!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365a0949",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 55m91mm7 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001880319350438253\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmlburnham\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\mikeb\\OneDrive - The Pennsylvania State University\\Stance Detection\\tutorials\\wandb\\run-20221003_005853-55m91mm7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href=\"https://wandb.ai/mlburnham/stance_sweep/runs/55m91mm7\" target=\"_blank\">toasty-sweep-1</a></strong> to <a href=\"https://wandb.ai/mlburnham/stance_sweep\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/mlburnham/stance_sweep/sweeps/ktzjgwry\" target=\"_blank\">https://wandb.ai/mlburnham/stance_sweep/sweeps/ktzjgwry</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c2e05c6b6214047b60635a640d20cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/697 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa4c2cb1fcb46939368a29c7e134ae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/515M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.agent(sweep_id, train, count = 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
